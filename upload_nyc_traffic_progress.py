from elasticsearch import Elasticsearch, helpers
import json
import urllib3
from tqdm import tqdm  # ‚úÖ hi·ªÉn th·ªã progress bar
import os

# ‚ö†Ô∏è B·ªè c·∫£nh b√°o SSL do d√πng ch·ª©ng ch·ªâ t·ª± k√Ω
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ‚öôÔ∏è K·∫øt n·ªëi t·ªõi 3 node trong cluster
es = Elasticsearch(
    hosts=[
        "https://127.0.0.1:9200",
        "https://127.0.0.1:9201",
        "https://127.0.0.1:9202"
    ],
    basic_auth=("elastic", "u3DXEPWQ7fp8z-Npn6fn"),
    verify_certs=False
)

INDEX_NAME = "nyc_traffic"

# üß± T·∫°o index n·∫øu ch∆∞a c√≥
if not es.indices.exists(index=INDEX_NAME):
    mapping = {
        "settings": {
        "number_of_shards": 6,
        "number_of_replicas": 1
    },
    "mappings": {
        "properties": {
            "id": {"type": "keyword"},
            "link_id": {"type": "integer"},
            "link_name": {"type": "text"},
            "borough": {"type": "keyword"},
            "speed": {"type": "float"},
            "travel_time": {"type": "float"},
            "data_as_of": {"type": "date"},
            "status": {"type": "integer"}
        }
    }
    }
    es.indices.create(index=INDEX_NAME, body=mapping)
    print(f"‚úÖ Created index '{INDEX_NAME}'")
else:
    print(f"‚ÑπÔ∏è Index '{INDEX_NAME}' already exists")

# üßÆ ƒê·∫øm s·ªë d√≤ng trong file ƒë·ªÉ setup progress bar
def count_lines(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

# üöÄ Generator ƒë·ªçc d·ªØ li·ªáu theo d√≤ng (√≠t t·ªën RAM)
def generate_actions(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            doc = json.loads(line)
            unique_id = f"{doc.get('id')}_{doc.get('data_as_of')}"  
            yield {
                "_index": INDEX_NAME,
                "_id": unique_id,
                "_source": doc
            }

# üì¶ Upload d·ªØ li·ªáu b·∫±ng Bulk API v·ªõi thanh ti·∫øn tr√¨nh
def bulk_upload(file_path):
    total_lines = count_lines(file_path)
    print(f"üìä File c√≥ kho·∫£ng {total_lines:,} d√≤ng\n")

    with tqdm(total=total_lines, unit="docs", desc="Uploading") as pbar:
        success, failed = 0, 0
        for ok, _ in helpers.streaming_bulk(
            es, 
            generate_actions(file_path),
            chunk_size=10000,  # upload 10k b·∫£n ghi/l·∫ßn
            request_timeout=300
        ):
            if ok:
                success += 1
            else:
                failed += 1
            pbar.update(1)
    print(f"\n‚úÖ Ho√†n t·∫•t: Uploaded = {success:,}, Failed = {failed:,}")

if __name__ == "__main__":
    file_path = r"E:\data json\nyc_traffic_all.ndjson"
    if os.path.exists(file_path):
        bulk_upload(file_path)
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu!")






